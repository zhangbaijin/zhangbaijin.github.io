<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<HTML xml:lang="en" xmlns="http://www.w3.org/1999/xhtml">
<META name=GENERATOR content="MSHTML 11.00.9600.17280"></HEAD>
<META name=keywords 
content="Qidong Huang, Huang Qidong, qidong huang, huang qidong, qidonghuang, huangqidong, 黄启栋, ustc, USTC, University of Science and Technology of China">
<META 
content="IE=7.0000" http-equiv="X-UA-Compatible">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <link rel="stylesheet" href="./jemdoc.css" type="text/css">
    <title>Qidong Huang's Homepage</title>
</head>



<body data-feedly-mini="yes">

<div id="layout-content" style="margin-top:25px">

<table>
    <tbody>
        <tr>
            <td width="670" >
                <div id="toptitle">                 
                    <h1> Qidong Huang (黄启栋) &nbsp; </h1><h1> <!-- <img src="./contents/portrait.jpg" width="190" style="margin-bottom:-10px"> -->
                </h1></div>

                <h3>Ph.D student, USTC
                </h3></div> 
                <br>
                <br>
                <p>
                    Information process center, CAS Key Laboratory of Electro-magnetic Space Information
                    <br>
                    <br>
                    <a href="http://cybersec.ustc.edu.cn/main.htm">School of Cyber Science and Technology</a>, <a href="https://www.ustc.edu.cn/">University of Science and Technology of China</a>
                    <br>
                    <br>
                    Email: hqd0037[AT]mail.ustc.edu.cn
                    <br>
                    <br>
                    <font color="#FF0000">Currently on the job market! Seeking faculty/job opportunities (begin in 2025 Summer/Fall)!</font>
                    <br>
                    <br>
                    [<a href="https://scholar.google.com/citations?user=F-OzLhQAAAAJ&hl=zh-CN">Google Scholar</a>]
                    [<a href="https://github.com/shikiw">GitHub</a>]
                    [<a href="./contents/CV_QidongHuang.pdf">CV</a>]
                    [<a href="https://twitter.com/qidong_huang">Twitter</a>] 
                </p>
            </td>
            <td rowspan="1">
                <img src="./contents/portrait.jpg" border="0" width="300" align="bottom" ><br>
            </td>
        </tr>
                <!-- <br> -->
                <!-- <colgroup><col height="50" width="270">
                </colgroup> -->
                <!-- <tbody> -->
                <td align="bottom" colspan="2" style="margin-top:2px">
                    <h2>News</h2>
                    <div style="height: 150px; overflow: auto;">
                    <ul style="margin-left:2px; padding-left:20px; margin-top:5px">
                        <li>10/2024: Check out our <a href="https://arxiv.org/pdf/2410.17247">PyramidDrop</a>, accelerating your LVLM with over 1.7X training speed and 2.0X inference speed!</li>
                        <li>10/2024: We introduce <a href="https://arxiv.org/abs/2410.07167">MIR&MoCa</a>, a LVLM pre-training indicator and a light-weight modality calibration module!</li>
                        <li>04/2024: <a href="https://arxiv.org/abs/2311.17911">OPERA</a> is selected as Highlight in CVPR 2024!</li>
                        <li>02/2024: Two papers are accepted by <a href="https://cvpr.thecvf.com"> CVPR 2024</a>. See you at Seattle! </li>
                        <li>02/2024: I have one paper accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83"> IEEE TIP 2024</a>. </li>
                        <li>12/2023: Please check out our new work <a href="https://arxiv.org/abs/2311.17911"> OPERA</a> for mitigating MLLM's hallucination! </li>
                        <li>07/2023: I have one paper accepted by <a href="https://www.acmmm2023.org/"> ACM MM 2023</a>. </li>
                        <li>07/2023: I have one paper accepted by <a href="https://iccv2023.thecvf.com/"> ICCV 2023</a>. See you at Paris! </li>
                        <li>07/2023: I have a new homepage. </li>
                    </ul>
                    </div>
                </td>
            

        </tbody>
</table>

<h2>About Me</h2>
<p>
    I am currently an Ph.D student at <a href="http://cybersec.ustc.edu.cn//">School of Cyber Science and Technology</a>,  <a href="https://www.ustc.edu.cn//"> University of Science and Technology of China </a>, working with Prof. <a href="http://staff.ustc.edu.cn/~ynh/">Nenghai Yu</a> and Prof. <a href="http://staff.ustc.edu.cn/~zhangwm/">Weiming Zhang</a>. Prior to that, I received my bachelor degree in <a href="http://cybersec.ustc.edu.cn//">School of Information Engineering</a>, <a href="https://www.ustc.edu.cn//"> University of Science and Technology of China</a>. My current research focuses on large vision-language models. 
</p> 
    

<h2>Biography</h2>

<li> 2023.08 - Present, Research Intern at <a href="https://www.shlab.org.cn/">Shanghai AI Lab</a>, supervised by <a href="https://myownskyw7.github.io/">Jiaqi Wang</a> and <a href="https://scholar.google.com/citations?user=FscToE0AAAAJ&hl=en">Xiaoyi Dong</a>.

<li> 2020.09 - Present, Ph.D. in <a href="http://cybersec.ustc.edu.cn//">School of Cyber Science and Technology</a>,  <a href="https://www.ustc.edu.cn//"> University of Science and Technology of China </a>.

<li> 2016.09- 2020.06, B.Eng. in <a href="http://cybersec.ustc.edu.cn//">School of Information Engineering</a>,  <a href="https://www.ustc.edu.cn//"> University of Science and Technology of China </a>


</tbody></table>

<!-- <h2>Preprints </a>  </h2> 
    <table id="Preprints" width="100%">
    <tbody>

    <tr>
        <td width="300">
        <img src="./images/opera.png" width="260px" height="120px">
        </td>
        <td>
        <p> OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation </a></p>
        <p><b>Qidong Huang</b>, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, Nenghai Yu</p>
        [<a href="https://arxiv.org/abs/2311.17911">arXiv</a>]
        [<a href="https://github.com/shikiw/OPERA">Code</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

</tbody></table> -->


<h2>Preprints </a>  </h2> 
    <table id="Preprints" width="100%">
    <tbody>

    <tr>
        <td width="300">
        <img src="./images/mir.png" width="260px" height="120px">
        </td>
        <td>
        <p> Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate </a></p>
        <p><b>Qidong Huang</b>, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Jiaqi Wang, Dahua Lin, Weiming Zhang, Nenghai Yu</p>
        [<a href="https://arxiv.org/abs/2410.07167">arXiv</a>]
        [<a href="https://github.com/shikiw/Modality-Integration-Rate">Code</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

    <tr>
        <td width="300">
        <img src="./images/pdrop.png" width="260px" height="120px">
        </td>
        <td>
        <p> PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction </a></p>
        <p>Long Xing, <b>Qidong Huang</b>, Xiaoyi Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang, Feng Wu, Dahua Lin</p>
        [<a href="https://arxiv.org/abs/2410.17247">arXiv</a>]
        [<a href="https://github.com/Cooperx521/PyramidDrop">Code</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

</tbody></table>


<h2>Publications </a>  </h2> 
<table id="tbPublications" width="100%">
    <tbody>

    <tr>
        <td width="300">
        <img src="./images/opera.png" width="260px" height="120px">
        </td>
        <td>
        <p> OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation </a></p>
        <p><b>Qidong Huang</b>, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, Nenghai Yu</p>
        <p><i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024. </i> <font color="#FF0000">(Highlight, 2.8% of submissions)</font> </p>
        [<a href="https://arxiv.org/abs/2311.17911">arXiv</a>]
        [<a href="https://github.com/shikiw/OPERA">Code</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

    <tr>    
        <td width="300">
            <img src="./images/tip2023.png" width="260px" height="120px">
            </td>
            <td>
        <p> PointCAT: Contrastive Adversarial Training for Robust Point Cloud Recognition  </a></p>
        <p><b>Qidong Huang</b>, Xiaoyi Dong, Dongdong Chen, Hang Zhou, Weiming Zhang, Kui Zhang, Gang Hua, Nenghai Yu</p>
        <p><i>IEEE Transactions on Image Processing (<b>TIP</b>), 2024. </i></p>
        [<a href="https://arxiv.org/abs/2209.07788">arXiv</a>]
        [<a href="https://github.com/shikiw/PointCAT">Code</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>
        
    <tr>
            <td width="300">
            <img src="./images/iccv2023.png" width="260px" height="120px">
            </td>
            <td>
            <p> Improving Adversarial Robustness of Masked Autoencoders via Test-time Frequency-domain Prompting </a></p>
            <p><b>Qidong Huang</b>, Xiaoyi Dong, Dongdong Chen, Yinpeng Chen, Lu Yuan, Gang Hua, Weiming Zhang, Nenghai Yu</p>
            <p><i>IEEE/CVF International Conference on Computer Vision (<b>ICCV</b>), 2023. </i></p>
            [<a href="https://arxiv.org/abs/2308.10315">arXiv</a>]
            [<a href="https://github.com/shikiw/RobustMAE">Code</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

    <tr>
            <td width="300">
            <img src="./images/cvpr2023.png" width="260px" height="120px">
            </td>
            <td>
            <p> Diversity-Aware Meta Visual Prompting </a></p>
            <p><b>Qidong Huang</b>, Xiaoyi Dong, Dongdong Chen, Weiming Zhang, Feifei Wang, Gang Hua, Nenghai Yu</p>
            <p><i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2023. </i></p>
            [<a href="https://arxiv.org/abs/2303.08138">arXiv</a>]
            [<a href="https://github.com/shikiw/DAM-VP">Code</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

    <tr>
            <td width="300">
            <img src="./images/cvpr2022.png" width="260px" height="120px">
            </td>
            <td>
            <p> Shape-invariant 3D Adversarial Point Clouds </a></p>
            <p><b>Qidong Huang</b>, Xiaoyi Dong, Dongdong Chen, Hang Zhou, Weiming Zhang, Nenghai Yu</p>
            <p><i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022. </i></p>
            [<a href="https://arxiv.org/abs/2203.04041">arXiv</a>]
            [<a href="https://github.com/shikiw/SI-Adv">Code</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

    <tr>
            <td width="300">
            <img src="./images/aaai2021.png" width="260px" height="120px">
            </td>
            <td>
            <p> Initiative Defense against Facial Manipulation </a></p>
            <p><b>Qidong Huang*</b>, Jie Zhang*, Wenbo Zhou, Weiming Zhang, Nenghai Yu (*Equal contribution)</p>
            <p><i>AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2021. </i></p>
            [<a href="https://arxiv.org/abs/2112.10098">arXiv</a>]
            [<a href="https://github.com/shikiw/initiative-defense-for-deepfake">Code</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

    <tr>
        <td width="300">
        <img src="./images/simac.png" width="260px" height="120px">
        </td>
        <td>
        <p> SimAC: A Simple Anti-Customization Method against Text-to-Image Synthesis of Diffusion Models </a></p>
        <p>Feifei Wang, Zhentao Tan, Tianyi Wei, Yue Wu, <b>Qidong Huang*</b> <font color="#FF0000">(Corresponding author)</font> </p>
        <p><i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024. </i></p>
        [<a href="https://arxiv.org/pdf/2312.07865.pdf">arXiv</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

    <tr>
            <td width="300">
            <img src="./images/mm2023.png" width="260px" height="120px">
            </td>
            <td>
            <p> Ada3Diff: Defending against 3D Adversarial Point Clouds via Adaptive Diffusion </a></p>
            <p>Kui Zhang, Hang Zhou, Jie Zhang, <b>Qidong Huang</b>, Weiming Zhang, Nenghai Yu</p>
            <p><i>ACM International Conference on Multimedia (<b>MM</b>), 2023. </i></p>
            [<a href="https://arxiv.org/abs/2211.16247">arXiv</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

    <tr>
            <td width="300">
            <img src="./images/tip2022.png" width="260px" height="120px">
            </td>
            <td>
            <p> Poison Ink: Robust and Invisible Backdoor Attack </a></p>
            <p>Jie Zhang, Dongdong Chen, <b>Qidong Huang</b>, Jing Liao, Weiming Zhang, Huamin Feng, Gang Hua, Nenghai Yu</p>
            <p><i>IEEE Transactions on Image Processing (<b>TIP</b>), 2022. </i></p>
            [<a href="https://arxiv.org/abs/2108.02488">arXiv</a>]
            [<a href="https://github.com/ZJZAC/Poison-Ink">Code</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

    <tr>
            <td width="300">
            <img src="./images/tcsvt2020.png" width="260px" height="120px">
            </td>
            <td>
            <p> Deep Template-based Watermarking </a></p>
            <p>Han Fang, Dongdong Chen, <b>Qidong Huang</b>, Jie Zhang, Zehua Ma, Weiming Zhang* and Nenghai Yu</p>
            <p><i>IEEE Transactions on Circuits and Systems for Video Technology (<b>TCSVT</b>), 2020. </i></p>
            [<a href="https://drive.google.com/file/d/17faFd2xQcFlDz0ZWUig7yqQpNB3XSC9i/view">Paper</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

 
</tbody></table>


<h2>Awards</h2>
<table id="Awards" border="0" width="100%">

    <tbody>
    <li> 2024 National Scholarship</a> 
    <li> 2021 National Scholarship</a> 
    <li> 2023 Anheng Information Scholarship</a> 

<tr><td>   <br>  </td></tr>


</tbody></table>

<h2>Services</h2>
<table id="Services" border="0" width="100%">

    <tbody>

Invited Talk: 
    <li> [2024.07] AI SPOT@OpenMMLab. Topic: "Exploring MLLM's Hallucination from A Causal Attention Perspective"

<br>
<br>

Invited Reviewer for: 
    <li> TPAMI, TNNLS, TIP, Pattern Recognition (PR)
    <li> CVPR, ICCV, ECCV, NeurIPS, ICLR


<tr><td>   <br>  </td></tr>

</tbody></table>


</body>
</html>
