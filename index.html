<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<HTML xml:lang="en" xmlns="http://www.w3.org/1999/xhtml">
<META name=GENERATOR content="MSHTML 11.00.9600.17280"></HEAD>
<META name=keywords 
content="Xiaofeng Zhang, Xiaofeng Zhang, Xiaofeng Zhang, zhang Xiaofeng, xiaofengzhang, zhangxiaofeng, 张晓峰, sjtu, SJTU, Shang Hai Jiao Tong Unversity">
<META 
content="IE=7.0000" http-equiv="X-UA-Compatible">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <link rel="stylesheet" href="./jemdoc.css" type="text/css">
    <title>Xiaofeng Zhang's Homepage</title>
</head>



<body data-feedly-mini="yes">

<div id="layout-content" style="margin-top:25px">

<table>
    <tbody>
        <tr>
            <td width="670" >
                <div id="toptitle">                 
                    <h1> Xiaofeng Zhang (张晓峰) &nbsp; </h1><h1> <!-- <img src="./contents/portrait.jpg" width="190" style="margin-bottom:-10px"> -->
                </h1></div>

                <h3>Ph.D student, SJTU
                </h3></div> 
                <br>
                <br>
                <p>
                    Information process center, CAS Key Laboratory of Electro-magnetic Space Information
                    <br>
                    <br>
                    <a href="http://cybersec.ustc.edu.cn/main.htm">School of Cyber Science and Technology</a>, <a href="https://www.ustc.edu.cn/">University of Science and Technology of China</a>
                    <br>
                    <br>
                    Email: framebreak@sjtu.edu.cn
                    <br>
                    <br>
                    <font color="#FF0000">Currently on the job market! Seeking faculty/job opportunities (begin in 2025 Summer/Fall)!</font>
                    <br>
                    <br>
                    [<a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=Y6Z5xQQAAAAJ">Google Scholar</a>]
                    [<a href="https://github.com/zhangbaijin">GitHub</a>]
                  
                
                </p>
            </td>
            <td rowspan="1">
                <img src="./contents/portrait.jpg" border="0" width="300" align="bottom" ><br>
            </td>
        </tr>
                <!-- <br> -->
                <!-- <colgroup><col height="50" width="270">
                </colgroup> -->
                <!-- <tbody> -->
                <td align="bottom" colspan="2" style="margin-top:2px">
                    <h2>News</h2>
                    <div style="height: 150px; overflow: auto;">
                    <ul style="margin-left:2px; padding-left:20px; margin-top:5px">
                        <li>10/2024: Check out our <a href="https://arxiv.org/html/2411.09968v1">Seeing Clearly by layer two</a>, Enhancing Attention Heads to Alleviate Hallucination in LVLMs!</li>
                        <li>10/2024: We introduce <a href="https://arxiv.org/html/2406.06579v3">LLaVA-CAM</a>, From Redundancy to Relevance: Information Flow in LVLMs Across Reasoning Tasks!</li>
                        <li>04/2024: <a href="https://arxiv.org/abs/2407.15130">DPORA</a> is selected in ACM MM2024 as poster!</li>
                        <li>02/2024: I have one paper accepted by <a href="https://wacv2025.thecvf.com/"> WACV 2025</a>. </li>
                        <li>02/2024: 2 papers are accepted by <a href="https://cvpr.thecvf.com"> ICONIP 2024</a> </li>
<!--                         <li>07/2023: I have one paper accepted by <a href="https://www.acmmm2023.org/"> ACM MM 2023</a>. </li>
                        <li>07/2023: I have one paper accepted by <a href="https://iccv2023.thecvf.com/"> ICCV 2023</a>. See you at Paris! </li>
                        <li>07/2023: I have a new homepage. </li> -->
                    </ul>
                    </div>
                </td>
            

        </tbody>
</table>

<h2>About Me</h2>
<p>
    I am currently an Ph.D student at SJTU</a>,  <a href="https://iwin.sjtu.edu.cn///"> Shang Hai Jiao Tong University </a>, working with Prof. <a href="https://automation.sjtu.edu.cn/Chao-Chen/">Chaochen Gu</a> and Prof. <a href="https://cs.pku.edu.cn/info/1082/3208.htm/">Hao Tang</a>. Prior to that, I received my bachelor degree in NJUPT</a>. My current research focuses on large vision-language models. 
</p> 
    

<h2>Biography</h2>

<li> 2024.01 - Present, Research Intern at Alibaba Groud(飞天实验室), supervised by <a href="https://scholar.google.co.jp/citations?user=b6vn1uMAAAAJ&hl=zh-CN/">Chen Shen.

<li> 2022.09 - Present, Ph.D at Shang Hai Jiao Tong Unversity.
    
<li> 2021.09 - 2022.9. Product manger. in China Mobile Communications Group Jiangsu Co., LTD. Wuxi branch.

<li> 2014.09- 2021.06, B/M.Eng. in Nanjing University of Posts and Telecommunications</a>


</tbody></table>

<!-- <h2>Preprints </a>  </h2> 
    <table id="Preprints" width="100%">
    <tbody>

    <tr>
        <td width="300">
        <img src="./images/opera.png" width="260px" height="120px">
        </td>
        <td>
        <p> OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation </a></p>
        <p><b>Xiaofeng Zhang</b>, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, Nenghai Yu</p>
        [<a href="https://arxiv.org/abs/2311.17911">arXiv</a>]
        [<a href="https://github.com/shikiw/OPERA">Code</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

</tbody></table> -->


<h2>Preprints </a>  </h2> 
    <table id="Preprints" width="100%">
    <tbody>

    <tr>
        <td width="300">
        <img src="./images/see-claerly.png" width="260px" height="120px">
        </td>
        <td>
        <p> Seeing Clearly by Layer Two: Enhancing Attention Heads to Alleviate Hallucination in LVLMs </a></p>
        <p><b>Xiaofeng Zhang</b>, Yihao Quan, Chaochen Gu, Chen Shen, Xiaosong Yuan, Shaotian Yan, Jieping Ye</p>
        [<a href="https://arxiv.org/html/2411.09968v1">arXiv</a>]

    </tr> 
    <tr><td>   <br>  </td></tr>

    <tr>
        <td width="300">
        <img src="./images/naacl.png" width="260px" height="120px">
        </td>
        <td>
        <p> From Redundancy to Relevance: Information Flow in LVLMs Across Reasoning Tasks </a></p>
        <p><b>Xiaofeng Zhang</b>, Yihao Quan, Chen Shen, Xiaosong Yuan, Shaotian Yan, Chaochen Gu, Hao Tang, Jieping Ye</p>
        [<a href="https://arxiv.org/html/2406.06579v3">arXiv</a>]
        [<a href="https://github.com/zhangbaijin/From-Redundancy-to-Relevance">Code</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

</tbody></table>


<h2>Publications </a>  </h2> 
<table id="tbPublications" width="100%">
    <tbody>

    <tr>
        <td width="300">
        <img src="./images/mm.png" width="260px" height="120px">
        </td>
        <td>
        <p> DOPRA: Decoding Over-accumulation Penalization and Re-allocation in Specific Weighting Layer </a></p>
        <p>Jingfeng Wei*, <b>Xiaofeng Zhang*</b></p>
        <p><b>ACM MM</b>, 2024. </i> </p>
        [<a href="https://arxiv.org/abs/2407.15130">arXiv</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

    <tr>    
        <td width="300">
            <img src="./images/wacv" width="260px" height="120px">
            </td>
            <td>
        <p> High-Fidelity Document Stain Removal via A Large-Scale Real-World Dataset and A Memory-Augmented Transformer  </a></p>
        <p>Mingxian Li, Hao Sun, Yingtie Lei,<b>Xiaofeng Zhang</b>, Yihang Dong, Yilin Zhou, Zimeng Li, Xuhang Chen</p>
        <p><i>WACV, 2024. </i></p>
        [<a href="https://arxiv.org/abs/2410.22922">arXiv</a>]
        [<a href="https://github.com/CXH-Research/StainRestorer">Code</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>
        
    <tr>
            <td width="300">
            <img src="./images/nips" width="260px" height="120px">
            </td>
            <td>
            <p> Instance-adaptive Zero-shot Chain-of-Thought Prompting</a></p>
            <p>Xiaosong Yuan, Chen Shen, Shaotian Yan, <b>Xiaofeng Zhang</b>, Liang Xie, Wenxiao Wang, Renchu Guan, Ying Wang, Jieping Ye</p>
            <p><i>NeurIPS, 2024. </i></p>
            [<a href="https://arxiv.org/abs/2409.20441">arXiv</a>]

    </tr> 
    <tr><td>   <br>  </td></tr>

    <tr>
            <td width="300">
            <img src="./images/cvpr2023.png" width="260px" height="120px">
            </td>
            <td>
            <p> Diversity-Aware Meta Visual Prompting </a></p>
            <p><b>Xiaofeng Zhang</b>, Xiaoyi Dong, Dongdong Chen, Weiming Zhang, Feifei Wang, Gang Hua, Nenghai Yu</p>
            <p><i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2023. </i></p>
            [<a href="https://arxiv.org/abs/2303.08138">arXiv</a>]
            [<a href="https://github.com/shikiw/DAM-VP">Code</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

    <tr>
            <td width="300">
            <img src="./images/cvpr2022.png" width="260px" height="120px">
            </td>
            <td>
            <p> Shape-invariant 3D Adversarial Point Clouds </a></p>
            <p><b>Xiaofeng Zhang</b>, Xiaoyi Dong, Dongdong Chen, Hang Zhou, Weiming Zhang, Nenghai Yu</p>
            <p><i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022. </i></p>
            [<a href="https://arxiv.org/abs/2203.04041">arXiv</a>]
            [<a href="https://github.com/shikiw/SI-Adv">Code</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

    <tr>
            <td width="300">
            <img src="./images/aaai2021.png" width="260px" height="120px">
            </td>
            <td>
            <p> Initiative Defense against Facial Manipulation </a></p>
            <p><b>Xiaofeng Zhang*</b>, Jie Zhang*, Wenbo Zhou, Weiming Zhang, Nenghai Yu (*Equal contribution)</p>
            <p><i>AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2021. </i></p>
            [<a href="https://arxiv.org/abs/2112.10098">arXiv</a>]
            [<a href="https://github.com/shikiw/initiative-defense-for-deepfake">Code</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

    <tr>
        <td width="300">
        <img src="./images/simac.png" width="260px" height="120px">
        </td>
        <td>
        <p> SimAC: A Simple Anti-Customization Method against Text-to-Image Synthesis of Diffusion Models </a></p>
        <p>Feifei Wang, Zhentao Tan, Tianyi Wei, Yue Wu, <b>Xiaofeng Zhang*</b> <font color="#FF0000">(Corresponding author)</font> </p>
        <p><i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024. </i></p>
        [<a href="https://arxiv.org/pdf/2312.07865.pdf">arXiv</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

    <tr>
            <td width="300">
            <img src="./images/mm2023.png" width="260px" height="120px">
            </td>
            <td>
            <p> Ada3Diff: Defending against 3D Adversarial Point Clouds via Adaptive Diffusion </a></p>
            <p>Kui Zhang, Hang Zhou, Jie Zhang, <b>Xiaofeng Zhang</b>, Weiming Zhang, Nenghai Yu</p>
            <p><i>ACM International Conference on Multimedia (<b>MM</b>), 2023. </i></p>
            [<a href="https://arxiv.org/abs/2211.16247">arXiv</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

    <tr>
            <td width="300">
            <img src="./images/tip2022.png" width="260px" height="120px">
            </td>
            <td>
            <p> Poison Ink: Robust and Invisible Backdoor Attack </a></p>
            <p>Jie Zhang, Dongdong Chen, <b>Xiaofeng Zhang</b>, Jing Liao, Weiming Zhang, Huamin Feng, Gang Hua, Nenghai Yu</p>
            <p><i>IEEE Transactions on Image Processing (<b>TIP</b>), 2022. </i></p>
            [<a href="https://arxiv.org/abs/2108.02488">arXiv</a>]
            [<a href="https://github.com/ZJZAC/Poison-Ink">Code</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

    <tr>
            <td width="300">
            <img src="./images/tcsvt2020.png" width="260px" height="120px">
            </td>
            <td>
            <p> Deep Template-based Watermarking </a></p>
            <p>Han Fang, Dongdong Chen, <b>Xiaofeng Zhang</b>, Jie Zhang, Zehua Ma, Weiming Zhang* and Nenghai Yu</p>
            <p><i>IEEE Transactions on Circuits and Systems for Video Technology (<b>TCSVT</b>), 2020. </i></p>
            [<a href="https://drive.google.com/file/d/17faFd2xQcFlDz0ZWUig7yqQpNB3XSC9i/view">Paper</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

 
</tbody></table>


<h2>Awards</h2>
<table id="Awards" border="0" width="100%">

    <tbody>
    <li> 2024 National Scholarship</a> 
    <li> 2021 National Scholarship</a> 
    <li> 2023 Anheng Information Scholarship</a> 

<tr><td>   <br>  </td></tr>


</tbody></table>

<h2>Services</h2>
<table id="Services" border="0" width="100%">

    <tbody>

Invited Talk: 
    <li> [2024.07] AI SPOT@OpenMMLab. Topic: "Exploring MLLM's Hallucination from A Causal Attention Perspective"

<br>
<br>

Invited Reviewer for: 
    <li> TPAMI, TNNLS, TIP, Pattern Recognition (PR)
    <li> CVPR, ICCV, ECCV, NeurIPS, ICLR


<tr><td>   <br>  </td></tr>

</tbody></table>


</body>
</html>
