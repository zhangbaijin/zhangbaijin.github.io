<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<HTML xml:lang="en" xmlns="http://www.w3.org/1999/xhtml">
<META name=GENERATOR content="MSHTML 11.00.9600.17280"></HEAD>
<META name=keywords 
content="Xiaofeng Zhang, Xiaofeng Zhang, Xiaofeng Zhang, zhang Xiaofeng, xiaofengzhang, zhangxiaofeng, sjtu, SJTU, Shang Hai Jiao Tong Unversity">
<META 
content="IE=7.0000" http-equiv="X-UA-Compatible">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <link rel="stylesheet" href="./jemdoc.css" type="text/css">
    <title>Xiaofeng Zhang's Homepage</title>
</head>



<body data-feedly-mini="yes">

<div id="layout-content" style="margin-top:25px">

<table>
    <tbody>
        <tr>
            <td width="670" >
                <div id="toptitle">                 
                    <h1> Xiaofeng Zhang &nbsp; </h1><h1> <!-- <img src="./contents/portrait.jpg" width="190" style="margin-bottom:-10px"> -->
                </h1></div>

                <h3>Ph.D student, SJTU
                </h3></div> 
                <br>
                <br>
                <p>
                   Key Laboratory of System Control and Information Processing
                    <br>
                    <br>
                         Shang Hai Jiao Tong Unversity</a>
                    <br>
                    <br>
                    Email: framebreak@sjtu.edu.cn, ÂæÆ‰ø°: SemiZxf
    <br>
    <br>
        Èí±Â°òÊ±ü‰∏äÊúù‰ø°Êù•Ôºå‰ªäÊó•ÊñπÁü•ÊàëÊòØÊàë
                    <br>
                    [<a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=Y6Z5xQQAAAAJ">Google Scholar</a>]
                    [<a href="https://github.com/zhangbaijin">GitHub</a>]


</p>
            </td>
            <td rowspan="1">
                <img src="./pic/handsome.jpg" border="0" width="300" align="bottom" ><br>
            </td>
        </tr>
<!--                 <br> -->
                <!-- <colgroup><col height="50" width="270">
                </colgroup> -->
                <!-- <tbody> -->
    
<!--                 <td align="bottom" colspan="2" style="margin-top:2px">
                    <h2>News</h2>
                    <div style="height: 150px; overflow: auto;">
                    <ul style="margin-left:2px; padding-left:20px; margin-top:5px">
                        <li>5/2025:  We have one paper (Image segmetation) accepted by <a href="https://icml.cc/Conferences/2025/"> ICML 2025</a>. </li>
                        <li>4/2025:  We have one paper (Image restoration) accepted by <a href="https://icml.cc/Conferences/2025/"> IJCAI 2025</a>. </li>
                        <li>1/2025:  I have one paper (LLaVA-CAM) accepted by <a href="https://arxiv.org/html/2406.06579v3/"> NAACL 2025 oral</a>. </li>
                        <li>1/2025:  We have one paper accepted by <a href="https://arxiv.org/html/2406.06579v3/"> ICLR 2025</a>. </li>
                        <li>1/2025:  I have one paper (Wakeup-Darkness) accepted by <a href="https://mc.manuscriptcentral.com/tomm/"> ACM TOMM</a>. </li>
                        <li>12/2024: I have one paper (Simignore) accepted by <a href="https://aaai.org/conference/aaai/aaai-25/"> Neural Network</a>. </li>
                        <li>12/2024: We have one paper accepted by <a href="https://ieeexplore.ieee.org/document/8101046/"> TIM</a>. </li>
                        <li>12/2024: I have one paper accepted by <a href="https://aaai.org/conference/aaai/aaai-25/"> AAAI 2025</a>. </li>
                        <li>10/2024: I have one paper accepted by <a href="https://wacv2025.thecvf.com/"> WACV 2025</a>. </li>
                        <li>09/2024: I have one paper accepted by <a href="https://neurips.cc/Conferences/2024/"> NIPS 2024</a>. </li>
                        <li>08/2024: I have one paper (DOPRA) accepted by <a href="https://2024.acmmm.org/"> ACM MM 2024</a>. </li>
                        <li>08/2024: I have one paper accepted by <a href="https://bmvc2024.org/"> BMVC 2024</a>. </li>
                        <li>01/2024: We have three paper accepted by <a href="https://2024.ieeeicassp.org/"> ICASSP 2024</a>. </li>
<!--                         <li>07/2023: I have one paper accepted by <a href="https://www.acmmm2023.org/"> ACM MM 2023</a>. </li>
                        <li>07/2023: I have one paper accepted by <a href="https://iccv2023.thecvf.com/"> ICCV 2023</a>. See you at Paris! </li>
                        <li>07/2023: I have a new homepage. </li> -->
                    </ul>
                    </div>
                </td>
    
        <td align="bottom" colspan="2" style="margin-top:2px">
                    <h2>News</h2>
                    <div style="height: 150px; overflow: auto;">
                    <ul style="margin-left:2px; padding-left:20px; margin-top:5px">
                        <li>9/2025:  We have one paper (Spatial-R1) accepted by <a href="https://neurips.cc/Conferences/2025/"> NeurIPS 2025 </a>, congratulations to Yifan Shen and Yuanzhe Liu.</li>
                        <li>8/2025:  I have one paper (EAH) accepted by <a href="https://2025.emnlp.org/"> EMNLP 2025 </a>oralüèÜ, see you in suzhou.</li>
                        <li>7/2025:  We have one paper (MCA-LLaVA) accepted by <a href="https://acmmm2025.org/"> ACM MM 2025</a>, congratulations to Qiyan Zhao.</li>
                        <li>6/2025:  We have one paper (pCR Prediction in Breast Cancer) accepted by <a href="https://conferences.miccai.org/2025/en/default.asp"> MICCAI 2025</a>, congratulations to Dingrui Ma.</li>
                        <li>6/2025:  We have one paper (AdaToken-3D, VLM token pruning ) accepted by <a href="https://iros25.org/m"> IROS 2025</a>, congratulations to Kai Zhang. </li>
                        <li>6/2025:  We have one paper (Mural inpainting) accepted by <a href="https://dl.acm.org/journal/tomm"> ACM TOMM</a>, congratulations to Zishan Xu. </li>
                        <li>5/2025:  We have one paper (Image segmetation) accepted by <a href="https://icml.cc/Conferences/2025/"> ICML 2025</a>, congratulations to Jiawei Cao. </li>
                        <li>4/2025:  We have one paper (Image restoration) accepted by <a href="https://icml.cc/Conferences/2025/"> IJCAI 2025</a>, congratulations to Jiesong Bai. </li>
                        <li>1/2025:  I have one paper (LLaVA-CAM) accepted by <a href="https://arxiv.org/html/2406.06579v3/"> NAACL 2025 oralüèÜ</a>. </li>
                        <li>1/2025:  We have one paper accepted by <a href="https://arxiv.org/html/2406.06579v3/"> ICLR 2025</a>, congratulations to Sinan Fan. </li>
                        <li>1/2025:  I have one paper (Wakeup-Darkness) accepted by <a href="https://mc.manuscriptcentral.com/tomm/"> ACM TOMM.</a>. </li>
                        <li>12/2024: I have one paper (Simignore) accepted by <a href="https://aaai.org/conference/aaai/aaai-25/"> Neural Network</a>. </li>
                        <li>12/2024: We have one paper accepted by <a href="https://ieeexplore.ieee.org/document/8101046/"> TIM</a>, congratulations to Jietao Yang. </li>
                        <li>12/2024: I have one paper accepted by <a href="https://aaai.org/conference/aaai/aaai-25/"> AAAI 2025</a>. </li>
                        <li>10/2024: We have one paper accepted by <a href="https://wacv2025.thecvf.com/"> WACV 2025</a>, congratulations to Yingtie Lei. </li>
                        <li>09/2024: We have one paper accepted by <a href="https://neurips.cc/Conferences/2024/"> NIPS 2024</a>, congratulations to Xiaosong Yuan. </li>
                        <li>08/2024: I have one paper (DOPRA) accepted by <a href="https://2024.acmmm.org/"> ACM MM 2024</a>. </li>
<!--                         <li>07/2023: I have one paper accepted by <a href="https://www.acmmm2023.org/"> ACM MM 2023</a>. </li>
                        <li>07/2023: I have one paper accepted by <a href="https://iccv2023.thecvf.com/"> ICCV 2023</a>. See you at Paris! </li>
                        <li>07/2023: I have a new homepage. </li> -->
                     </ul>
                    </div>
                </td>
            
            

        </tbody>
</table>
    
<h2>About Me</h2>
<!-- <p>
    I am currently an Ph.D student at SJTU</a>,  <a href="https://iwin.sjtu.edu.cn///"> Shang Hai Jiao Tong University </a>, working with Prof. <a href="https://automation.sjtu.edu.cn/Chao-Chen/">Chaochen Gu</a> and Prof. <a href="https://cs.pku.edu.cn/info/1082/3208.htm/">Hao Tang</a>. Prior to that, I received my bachelor degree in NJUPT</a>. My current research focuses on large vision-language models. 
</p>  -->
 <p>
    I am currently an third-year Ph.D student at SJTU</a>,  <a href="https://iwin.sjtu.edu.cn///"> Shang Hai Jiao Tong University </a>. Prior to that, I received my master degree in NJUPT</a>. My current research focuses on large vision-language models. 
</p>    

<h2> Biography</h2>

<li> 2022.09 - Present, Ph.D at Shang Hai Jiao Tong Unversity, supervised by Chaochen Gu and </a><a href="https://scholar.google.co.jp/citations?user=9zJkeEMAAAAJ&hl=zh-CN"></a> Hao Tang

<li> 2024.01 - 2025.7. Research Intern at Alibaba Cloud(È£ûÂ§©ÂÆûÈ™åÂÆ§), supervised by </a><a href="https://scholar.google.co.jp/citations?user=b6vn1uMAAAAJ&hl=zh-CN/"></a> Chen Shen.

<li> 2021.09 - 2022.9. Product manger. in China Mobile Communications Group Jiangsu Co., LTD. Wuxi branch.

<li> 2014.09- 2021.06, B/M.Eng. in Nanjing University of Posts and Telecommunications</a>


</tbody></table>



<h2>Publications</h2> 
<table id="tbPublications" width="100%">
    <tbody>


<tr>
            <td width="300">
            <img src="./images/spatialR1.png" width="260px" height="120px">
            </td>
            <td>
            <p>Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs</a></p>
            <p>Yifan Shen, Yuanzhe Liu, Jingyuan Zhu, Xu Cao, <b>Xiaofeng Zhang</b>, Yixiao He, Wenming Ye, James Matthew Rehg, Ismini Lourentzou</p>
            <p><i>NeurIPS, 2025 (projector leader). </i></p>
     [<a href="https://arxiv.org/pdf/2506.21656">Arxiv</a>]
           </tr> 
    <tr><td>   <br>  </td></tr> 

    
    <tr>
        <td width="300">
        <img src="./images/seeing.jpg" width="260px" height="120px">
        </td>
        <td>
        <p> Seeing Clearly by Layer Two: Enhancing Attention Heads to Alleviate Hallucination in LVLMs </a></p>
        <p><b>Xiaofeng Zhang*</b>, Yihao Quan*, Chaochen Gu, Chen Shen, Xiaosong Yuan, Shaotian Yan, Jieping Ye</p>
     <p><i>EMNLP 2025 OralüèÜ</i> </p>
        [<a href="https://arxiv.org/html/2411.09968v1">Arxiv</a>]
     [<a href="https://github.com/itsqyh/Shallow-Focus-Deep-Fixes">Code</a>]

    </tr> 
    <tr><td>   <br>  </td></tr> 


        
 <tr>
        <td width="300">
        <img src="./images/information-flow.png" width="260px" height="120px">
        </td>
        <td>
        <p> MCA-LLaVA: Manhattan Causal Attention for Reducing Hallucination in Large Vision-Language Models </a></p>
        <p>Qiyan Zhao*, <b>Xiaofeng Zhang*</b>, Yiheng Li, Yun Xing, Xiaosng Yuan, Feilong Tang, Sinan Fan, Xuhang Chen, Xuyao Zhang, Dahan Wang</p>
     <p><i>ACM MM 2025 (corresponding author/projector leader)</i> </p>
     [<a href="https://arxiv.org/abs/2507.09184">Arxiv</a>]
        [<a href="https://github.com/ErikZ719/MCA-LLaVA">Code</a>]

    </tr> 
    <tr><td>   <br>  </td></tr> 

        
   <tr>
        <td width="300">
        <img src="./images/naacl.jpg" width="260px" height="120px">
        </td>
        <td>
        <p> From Redundancy to Relevance: Information Flow in LVLMs Across Reasoning Tasks </a></p>
        <p><b>Xiaofeng Zhang</b>, Yihao Quan, Chen Shen, Xiaosong Yuan, Shaotian Yan, Chaochen Gu, Hao Tang, Jieping Ye</p>
        <p><i>NAACL  2025 OralüèÜ</i> </p>
        [<a href="https://arxiv.org/html/2406.06579v3">arXiv</a>]
        [<a href="https://github.com/zhangbaijin/From-Redundancy-to-Relevance">Code</a>]
    
    </tr> 
    <tr><td>   <br>  </td></tr>

        
   <tr>
        <td width="300">
        <img src="./images/simignore.jpg" width="260px" height="120px">
        </td>
        <td>
        <p>Enhancing Multimodal Large Language Models Complex Reasoning via Similarity Computation </a></p>
        <p><b>Xiaofeng Zhang</b>, Fanshuo Zeng, Yihao Quan, Zheng Hui, Jiawei Yao</p>
        <p><i>AAAI, 2025 </i> </p>
        [<a href="https://github.com/FanshuoZeng/Simignore">Code</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

      <tr>
        <td width="300">
        <img src="./images/clustering.png" width="260px" height="120px">
        </td>
        <td>
        <p>Simignore:Enhancing Multimodal Large Language Models Complex Reasoning via Similarity Computation </a></p>
        <p><b>Xiaofeng Zhang</b>, Fanshuo Zeng, Chaochen Gu</p>
        <p><i>Neural Network 2025 </i> </p>
        [<a href="https://github.com/FanshuoZeng/Simignore">Code</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>
        
    <tr>
        <td width="300">
        <img src="./images/mm.jpg" width="260px" height="120px">
        </td>
        <td>
        <p> DOPRA: Decoding Over-accumulation Penalization and Re-allocation in Specific Weighting Layer </a></p>
        <p>Jingfeng Wei*, <b>Xiaofeng Zhang*</b></p>
        <p><i>ACM MM, 2024üèÜ, (corresponding author/projector leader)</i> </p>
        [<a href="https://arxiv.org/abs/2407.15130">arXiv</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

  
        
  <tr>
            <td width="300">
            <img src="./images/iclr.png" width="260px" height="120px">
            </td>
            <td>
            <p>Improving complex reasoning with dynamic prompt corruption: a soft prompt optimization approch</a></p>
            <p>Sinan Fan, Liang Xie, Chen Shen, Ge Teng, Xiaosong Yuan, <b>Xiaofeng Zhang</b>, Jieping Ye</p>
            <p><i>ICLR, 2025. </i></p>

    </tr> 
    <tr><td>   <br>  </td></tr>
    
    <tr>
            <td width="300">
            <img src="./images/nips.jpg" width="260px" height="120px">
            </td>
            <td>
            <p> Instance-adaptive Zero-shot Chain-of-Thought Prompting</a></p>
            <p>Xiaosong Yuan, Chen Shen, Shaotian Yan, <b>Xiaofeng Zhang</b>, Liang Xie, Wenxiao Wang, Renchu Guan, Ying Wang, Jieping Ye</p>
            <p><i>NeurIPS, 2024. </i></p>
            [<a href="https://arxiv.org/abs/2409.20441">arXiv</a>]

    </tr> 
    <tr><td>   <br>  </td></tr>

    <tr>
            <td width="300">
            <img src="./images/tomm.jpg" width="260px" height="120px">
            </td>
            <td>
            <p>Wakeup-Darkness: When Multimodal Meets Unsupervised Low-light Image Enhancement</a></p>
            <p><b>Xiaofeng Zhang</b>, Zishan Xu, Hao Tang, Chaochen Gu, Wei Chen</p>
            <p><i>ACM Transactions on Multimedia Computing, Communications, 2025 </i></p>
            [<a href="https://ieeexplore.ieee.org/document/10440368">arXiv</a>]
            [<a href="https://github.com/zhangbaijin/Wakeup-Darkness">Code</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>


    
    <tr>
        <td width="300">
        <img src="./images/memorynet.jpg" width="260px" height="120px">
        </td>
        <td>
        <p> Memory augment is All You Need for image restoration </a></p>
        <p><b>Xiaofeng Zhang</b>, Chaochen Gu, Shanying Zhu</p>
    <p><i> IEEE Transactions on Consumer Electronics,  2025. </i></p>
        [<a href="https://arxiv.org/abs/2309.01377">arXiv</a>]
        [<a href="https://github.com/zhangbaijin/MemoryNet">Code</a>]
    
    </tr> 
    <tr><td>   <br>  </td></tr>
    
    <tr>
            <td width="300">
            <img src="./images/tetci.jpg" width="260px" height="120px">
            </td>
            <td>
            <p> MuralDiff:Diffusion for Ancient Murals restoration on Large-scale Pre-training</a></p>
            <p>Zishan Xu,<b>Xiaofeng Zhang</b>, Wei Chen, Jueting Liu, Tingting Xu, Zehua Wang</p>
            <p><i>IEEE Transactions on Emerging Topics in Computational Intelligence(<b>TETCI</b>), 2024. </i></p>
            [<a href="https://ieeexplore.ieee.org/document/10440368">arXiv</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>


    
    <tr>
            <td width="300">
            <img src="./images/spa-former.jpg" width="260px" height="120px">
            </td>
            <td>
            <p> SpA-Former: An Effective and lightweight Transformer for image shadow removal </a></p>
            <p><b>Xiaofeng Zhang</b>, Yudi Zhao, Chaochen Gu</p>
            <p><i>International Joint Conference on Neural Networks, (<b>IJCNN</b>), 2023. </i></p>
            [<a href="https://ieeexplore.ieee.org/document/10191081">arXiv</a>]
            [<a href="https://github.com/zhangbaijin/SpA-Former-shadow-removal">Code</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>


    <tr>
            <td width="300">
            <img src="./images/spl.jpg" width="260px" height="120px">
            </td>
            <td>
            <p>Sienet: Siamese expansion network for image extrapolation</a></p>
            <p><b>Xiaofeng Zhang</b>, Feng Chen, Cailing Wang</p>
            <p><i>IEEE Signal Processing Letters (<b>SPL</b>), 2021. </i></p>
            [<a href="https://arxiv.org/abs/2007.03851">Paper</a>]
     [<a href="https://github.com/nanjingxiaobawang/SieNet-Image-extrapolation">Code</a>]
    </tr> 
    <tr><td>   <br>  </td></tr>

 
</tbody></table>


<h2>Awards</h2>
<table id="Awards" border="0" width="100%">

    <tbody>

    <li> 2021 National Scholarship</a> 


<tr><td>   <br>  </td></tr>


</tbody></table>

<h2>Services</h2>
<table id="Services" border="0" width="100%">

    <tbody>


<br>
<br>

Invited Reviewer for: 
    <li> TIP, TETCI, IPM
    <li> NIPS, CVPR, ICLR, AAAI, IJCAI, ACM MM, ACL, EMNLP


<tr><td>   <br>  </td></tr>

</tbody></table>


</body>
</html>




















